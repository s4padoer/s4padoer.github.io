---
title: "COPOD: Why I think that copula based outlier detection is awesome"
date: 2022-8-14
mathjax: true
categories:
  - blog
tags:
  - COPOD, outlier detection, methods
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" defer
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


{% if page.mathjax %}
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });


  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
    alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
    alert("Math Processing Error: "+message[1]);
    });
</script>
{% endif %}

# Introduction

It was again my job - and a very talented colleague of mine - that rose my interest in outlier detection. There are several ways to approach the term 'outlier'. Surely, one approach that is also aligned with the semantics of the word is a proximity-based understanding: Observations that are metrically far apart, i.e. 'lying far away' from the majority of other observations could be considered to be outliers. Coming from applied statistics, though, I do prefer another approach to outliers, the probability-based approach. In this sense, outliers are observations, that occur much more rarely than others. I prefer this approach first for the reason that measurable spaces (as they are required in probability theory) are not the same as metric spaces (I guess, though, that the most use cases in applied statistics are metric measure spaces and thus in practice, where measures are induced by metrics, this point does not play a role). Second, the correct interpretation of 'far apart' differs from a random variable's underlying distribution: If you have a variable that lives in the unit interval, a distance of 1 is much larger than for a Gaussian random variable with a relatively large variance and two points with distance 1 around the distribution's mean. For a probability-based approach, though, an observation deviating from the other 99% is always 'a stranger' no matter whether the actual distance is 1 or 100. Amongst the probability-based outlier detection (OD) methods, a relatively new, computationally efficient (!) tool that does not even require strong theoretical assumptions is the ([copula based (COPOD) approach by Zhao et al. (2020)](https://www.researchgate.net/profile/Yue-Zhao-70/publication/344306968_COPOD_Copula-Based_Outlier_Detection/links/5f67801492851c14bc899dd6/COPOD-Copula-Based-Outlier-Detection.pdf?origin=publication_detail)).

One final note before we start to take a closer look on COPOD: Outlier detection is a useful tool to identify observations that are worth to take a closer look into. I do not agree with Zhao et al. (2020) who state that outliers need to be removed (at least not unconditional to the circumstances): Assume you want to analyze the impact of family education over generations on wealth. It then may make sense to keep e.g. Bill Gates in your dataset although he is clearly an outlier, but generated his wealth thanks to the education he was provided. A lottery winner on the other hand is an outlier that you might want to remove from your dataset in that case.

# Copulae

The hassle with probability-based outliers in high-dimensional datasets starts when we cannot assume that the different dimensions of a multivariate random variable are independent. In this case, we cannot simply multiply the respective density/ probability distributions to get the joint distribution. Also the construction of the cumulative density/ probability function then becomes more complicated (and in the case of empirical data, we may not have enough data points to get a sufficiently granular empirical cumulative distribution function (cdf)). A workaround for these problems are copula functions. Briefly, a copula is a probability distribution $P_C$ for a $d$-dimensional real valued random variable $U = (U_1,\dotsc,U_d)$ such that each marginal cdf of each $U_i$, is uniform in the unit interval, i.e. 

$$
P_C(U_i \leq u) := \begin{cases}
1, \text{ if } u \geq 1, \\
u, \text{ if } u \in (0,1), \\
0, \text{ else }
\end{cases} \forall i \in \{1,\dotsc,d\} \quad.
$$

You might ask: What is the use of copulae? Rarely, any data follows such a distribution function! You are right. However, Zhao et al. (2020) say that Sklar (1959) showed that for each multivariate real-valued random vector $X = (X_1,\dotsc,X_d)$ with marginal distribution $F_1, \dotsc, F_d $ there exists a distribution (specifically, copula) $P_C$ such that for the joint cdf $F$ we have

$$
F(X) = P_C\left( F_1(X_1),\dotsc, F_d(X_d) \right) \quad.
$$

What does this mean? Well, with the definition above this means that for each $i = ,1\dotsc,d$, we have that $P_C( F_i(X_i) \leq u) = u$ for each $u \in [0,1]$. This is comfortable because of two reasons: First, from a uniform distribution of a transformed variable, it is easy to generate the original distribution via the inverse image, if $X_i \sim F_i$ and $P_C(F_i(X_i)) \sim U[0,1]$, then $X_i \sim F_i^{-1}P_C^{-1}(U)$ where $U$ is uniformly distributed on the unit interval. Second, we can write the joint distribution $F$ as a function of marginals. Marginals are very convenient because to get a stable estimate of them, we can use the complete data series and do not need to take into account the realizations of the other elements in the random vector. Thus, we can also easily get $F_i^{-1}$.

Assume now that we are interested in the probability that $F(X \leq x)$. We get with the existence of $P_C$:

$$
F(X \leq x) = P_C\left( F_1(X_1 \leq x_1),\dotsc, F_d(X_d \leq x_d ) \right) 
$$ 

that is, if we know $P_C$ and $F_1,\dotsc,F_d$ (of which we can get - as already mentioned - easily estiamtes), we can also get the (estimated) probability. If we want a probability based outlier detection, we mean to search for rare events, i.e. (in a simplified way) $x$ such that $F(X \leq x)$ or $F(X > x)$ is small. So let's find our (empirical) $P_C$!

# Estimating the joint distribution function
As already mentioned, it is easy to get estimates of the marginal cdfs of the random variables. Simply use the estimator

$$
\hat{F}_i(x) := \frac{1}{n} \sum_{j = 1}^n \mathbb{1}\left( X_{i,j} \leq x\right)
$$

for a set of random realizations $j = 1,\dotsc,n$ of the random vector $X = (X_1,\dotsc,X_i,\dotsc,X_d)$. We can then set the vector of random variables $\left(\hat{U}_1,\dotsc, \hat{U}_d \right) := \left( \hat{F}_1(X_1), \dotsc, \hat{F}_d(X_d) \right)$. That is, given a realization $x$ of $X$ the vector of marginal cumulative probabilities is the outcome of $U$. As we have $j = 1,\dotsc,n$ independent realizations of $X$, we can calculate the vector $u$. What do we do with $u$? From our expression for the copula

$$
P_C( U \leq u) = \mathrm{Prob}\left( F_1(X_1) \leq u_1, \dotsc, F_d(X_d) \leq u_d \right) = \mathrm{Prob}\left( U_1 \leq u_1, \dotsc, U_d \leq u_d \right)
$$

and the classical empirical cdf estimator, we get the copula estimator

$$
\hat{P}_C( U \leq u) = \frac{1}{n} \sum_{j = 1}^n \mathbb{1}\left( U_1 \leq u_1 \land \dotsc \land U_d \leq u_d \right) \quad.
$$

This copula estimator is now essential because we have - as you surely remember - $F(X) = P_C\left( F(X_1), \dotsc, F(X_d) \right)$. We thus can estimate tail probabilities with the help of the empirical copula.

# Outlier detection

Of course, you could think of cases where rare events lie in the middle of a random variable's range. This however, does not correspond to the proximity based definition of outliers that we discussed in the beginning. (And I'd assume that this is also much apart from classical use cases and the common understanding - e.g. you do not say that someone is extremely average intelligent but outliers are either brilliant or stupid.) So, if we want to combine the proximity and the probability-based approach to outlier detection, we head for the tail probabilities, i.e. $F(x) = \mathrm{Prob}\left( X \leq x\right)$ or $1-F(x) = \mathrm{Prob}(X > x)$ for a realization $X = x$. We can then tell the estimated probability that a random variable takes a value that is more or as extreme than a realization $x_j$ from the sample, i.e. $X \leq x_j$, using the calculated values $\hat{F}(x_{j}) = \hat{P}_C( u_{j} ) \approx u_{i,j}$. Note that we can get $X \geq x$ when calculating the marginal cdfs of $-X$. If the probability for $X \leq x_{j}$ or $X\geq x_j$ is sufficiently small, we may call the realization an outlier.

# Final notes
Zhao et al. (2020) set $P_C\left((u_1,\dotsc,u_d)\right) = \prod_i P_C(u_i)$. I assume this due to the fact that the copula is a function of marginals. If two random variables are independent, their joint probability distribution is the product of marginals and the other way around. Unfortunately, the original paper is not accessible online, so I do not know whether there is something stated about the independence for the copula. If, for example, $Y_1 \sim U[0,1]$ and $Y_2 = Y_1$, then their marginal cdfs are the same (and those of a uniform distribution like we need for a copula). However, the joint cdf is not equal to the product because the random variables are not independent. 

Second, if we have skewed random varialbes e.g. with a probability density distribution like here

<figure>
    <a href="/assets/images/lognormal.png"><img src="/assets/images/lognormal.png"></a>
    <figcaption>Log-normal probability density distribution</figcaption>
</figure>

it makes sense only to take e.g. the right tail into account (or the left tail for left-skewed random variables). Zhao et al. (2020) do this by the 'skewness corrected' copula that has as factors in the product either the probability for $X_i > x_i$ or $X_i < x_i$ depending on whether this element in the random vector is right- or left-skewed.

Third, in high dimensional settings, we would relatively quickly reach values close to zero for the copula because probabilities even for unbounded random variables must decrease from a certain value on exponentially. To circumvent this problem, Zhao et al. (2020) suggest to take the negative logartihmic probabilities as a measure for outliers.


# References
A. Sklar, “Fonctions de repartition an dimensions et leurs marges,” Publ. inst. statist. univ. Paris, vol. 8, pp. 229–231, 1959.

Y. Zaho, N. Botta, C. Ionescu and X. Hu, "COPOD: Copula-Based Outlier Detection", Conference Paper, DOI: 10.1109/ICDM50108.2020.00135, 2020.
