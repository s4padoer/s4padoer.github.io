---
title: "HDBSCAN: The idea of hierarchical density based clustering"
date: 2022-4-23
mathjax: true
categories:
  - blog
tags:
  - HDBSCAN, unsupervised clustering, methods
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" defer
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


{% if page.mathjax %}
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });


  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
    alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
    alert("Math Processing Error: "+message[1]);
    });
</script>
{% endif %}

# Introduction

For my job, I have come in touch with HDBSCAN, a density based clustering algorithm. I found it to work formidably well so I want to outline here the basic working of the algorithm. I have not found too many nice explainations of the method and the intuition in the internet besides the ([Python API guide](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)) the very long and detailed seminar paper ([https://dl.acm.org/doi/pdf/10.1145/2733381](https://dl.acm.org/doi/pdf/10.1145/2733381)) in which it was introduced. For this reason, I decided to write a post on this.

The idea of HDBSCAN is actually to make clustering algorithms like single linkage and DBSCAN more flexible: Single linkage is noise-sensitive, as two clusters with a noise data point in-between may be identified as a single cluster because the noise is interpreted as a point in the middle of the larger, unified cluster. DBSCAN, on the other hand, can identify noise, but on the other hand, the algorithm is not capable to identify areas with 'higher-than-noise-density' that have still varying density levels. 

# The role of mutual reachability

For HDBSCAN, the definition of mutual reachability is essential: The mutual reachability is built from the core distance $d_{\mathrm{core},k}$ of the $k$-nearest neighbours of a point $x$ and another point $y$ (both in the space $\mathbb{R}^d$ under a distance measure $d$.

$$
d_{\mathrm{mreach},k}(x,y) := \max\left\{ d_{\mathrm{core},k}(x), d_{\mathrm{core},k}(y), d(x,y) \right\}
$$

As a short reminder, the core distance for $k$ nearest neighbours of $x$ is the distance between $x$ and $x$'s $k$th closest data point (including $x$ as well). For the equation-philes amongst us, this means 

$$
d_{\mathrm{core},k}(x) = \mathrm{arg}\,\min_\epsilon \{|N_\epsilon(x)| = k\}.
$$

What is the purpose of the mutual reachability distance? The core distance is an easy way to estimate the density around a data point: The smaller the core distance, the higher the density around the point $x$. The larger $k$, the more robust is the core distance (and thus the density estimator) to noise: If two points, close to each other, are far apart from the rest of the sample of, say, 100 data points, $k = 2$ would return a high density estimator whilst $k = 3$ would correctly return a low density area.

So why should one refine the core distance for density-based clustering? We could classify data points as noise for a fixed $\epsilon$ if $d_{\mathrm{core,k}}(x) > \epsilon$. If, on the other hand, data points $x$ and $y$ have a core distance smaller than $\epsilon$ and are in the respective $\epsilon$-neighbourhood, they are not only non-noise, but also in the same cluster. This however, makes the definition of noise and clusters highly dependent on the choice of $\epsilon$. This is the short-coming of DBSCAN. If we want to keep track of minimal necessary $\epsilon$ values to put $x$ and $y$ into the same cluster (we define a decision rule for this later in the post), the mutual reachability is the right choice: It gives us exactly this $\epsilon$ value. 

Based on the mutual reachability, we can construct a weighted, undirected, complete graph, the *mutual reachability graph*. Then, we can make the link to single linkage by constructing a minimum spanning tree (MST) and then removing edges from the MST according to a rule explained in the next section so that we have different connected components. 

# Extracting a hierarchical clustering and a flat clustering

Intuitively, it makes sense to cluster points that have a mutually good reachability and assign a different cluster label to a group of points that are apart from such a group. In order to maintain amongst the nodes in the components a minimal summed reachability distance, it makes furthermore sense to remove edges of a MST by decreasing reachability distance order. A node is then considered to be noise at reachability level $\epsilon$ if it becomes an isolated node after removing an edge with distance weight $\epsilon$.

We can thus easily retrieve an hierarchical clustering. But how do we get a flat clustering and determine the relevant level(s) of $\epsilon$ to define noise data? Of course, we could simply fix $\epsilon$ and define everything with a reachability distance as noise and set connected components above this threshold as clusters. This is what DBSCAN does. However, I have already mentioned that this is somewhat inflexible with respect to the density level, take for example:

<figure>
    <a href="/assets/images/gaussian_mixture.png"><img src="/assets/images/gaussian_mixture.png"></a>
    <figcaption>A Gaussian mixture model with two components</figcaption>
</figure>

For this reason, HDBSCAN does something more elaborate: From the dendrograms, the concept of a 'cluster lifetime' is adopted: Depending on the expected excess of mass, an indicator for cluster stability is derived. The (relative) excess of mass of a cluster $C_i$ is defined as

$$
\mathrm{E}_R(C_i) = \int_{C_i} \left(\lambda_{\text{max}}(x, C_i) - \lambda_{\text{min}}(C_i)\right) \mathrm{d} \, x
$$

where $\lambda_{\text{max}}(x, C_i) := \min\left\\{ f(x), \lambda_{\text{max}}(C_i)\right\\}$ and $f$ is the density at data point $x$ and $\lambda_{\text{max}}(C_i)$ is the density level at which cluster $C_i$ is either split into distinct clusters or disappears. We take as density level the inverse of $\epsilon$ from the mutual reachability distance that we kept track of. Inversely, $\lambda_{\text{min}}(C_i)$ is the density level at which $C_i$ merges into a larger cluster. An area represents a cluster when it is density connected at a given density level.

The relative excess of mass has the advantage over the classical excess of mass as defined in the literature (for the mathematicians, this is $\mathrm{E}(C_i) :=  \int_{C_i} \left(f(x) - \lambda_{\text{min}}(C_i)\right) \mathrm{d}\, x$ :) ) that it allows to compare nested clusters: Assume that $C_j \subset C_i$ and $C_j \neq C_i$. In that case, $\mathrm{E}(C_i) > \mathrm{E}(C_j)$ and for this reason, the larger is always considered to be stabler - thus always favoring the larger cluster. As we have $\lambda_{\text{max}}(C_j) = \lambda_{\text{min}}(C_i)$ and for $x \in C_j$, $f(x) \leq \lambda_{\text{max}}(C_j) \leq \lambda_{\text{max}}(C_i)$, we do not have necessarily larger stability for $C_i$ than for $C_j$ using the relative excess of mass.

The aim is now to maximize the overall stability of the clustering selecting a subset $J$ of all the identified (potentially nested) clusters under the constraint, that $\left\\{C_j\right\\}_{j\in J}$ is a partition of the domain. This is a discrete optimization problem about which one could write a post itself. 


